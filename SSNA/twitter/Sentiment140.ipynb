{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment140 Dataset (as taken from Kaggle):\n",
    "The current dataset comprises of 1.6 million tweets with a sentiment value of 0 (negative), 2 (neutral) or 4 (positive). We will try to find tweets with mentions interconnecting users to build a directed graph.\n",
    "\n",
    "The zip file can be downloaded from https://www.kaggle.com/kazanova/sentiment140/downloads/sentiment140.zip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The imported dataset has a total of 1600000 tweets.\n"
     ]
    }
   ],
   "source": [
    "base_dataframe = pd.read_csv(\"sentiment140.zip\", delimiter=',', encoding='latin-1', compression='zip', header=None)\n",
    "print(\"The imported dataset has a total of\", len(base_dataframe), \"tweets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets which aren't neutral are 1600000 in number.\n",
      "Thereby, all tweets in the dataset are either positive or negative.\n"
     ]
    }
   ],
   "source": [
    "biased_dataframe = base_dataframe.loc[base_dataframe[0] != 2]\n",
    "print(\"Tweets which aren't neutral are\", len(biased_dataframe), \"in number.\")\n",
    "print(\"Thereby, all tweets in the dataset are either positive or negative.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 659775 distinct source users who tweeted a total of 1600000 tweets.\n"
     ]
    }
   ],
   "source": [
    "src_users = biased_dataframe[4].values\n",
    "src_users_set = set(src_users)\n",
    "print(\"There are\", len(src_users_set), \"distinct source users who tweeted a total of\", len(biased_dataframe), \"tweets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 703311 tweets directed to single other users.\n",
      "There are a total of 33194 tweets directed to multiple other users.\n"
     ]
    }
   ],
   "source": [
    "tweets = biased_dataframe[5].values\n",
    "uni = []\n",
    "multi = []\n",
    "for tweet in tweets:\n",
    "    words = tweet.split()\n",
    "    uniflag = False\n",
    "    multiflag = False\n",
    "    for word in words:\n",
    "        if word[0] == '@' and len(word) > 1 and not uniflag:\n",
    "            uniflag = True\n",
    "        elif word[0] == '@' and len(word) > 1 and uniflag:\n",
    "            multiflag = True\n",
    "    if multiflag:\n",
    "        multi.append(tweet)\n",
    "    elif uniflag:\n",
    "        uni.append(tweet)\n",
    "print(\"There are a total of\", len(uni), \"tweets directed to single other users.\")\n",
    "print(\"There are a total of\", len(multi), \"tweets directed to multiple other users.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function looks for targetted tweets and isolated them into two sets. Tweets directed at one user and tweets directed at multiple users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_directed_tweets(src_list, tweet_list, sentiment_list):\n",
    "    unidirected = []\n",
    "    multidirected = []\n",
    "    for index, tweet in enumerate(tweet_list):\n",
    "        target = []\n",
    "        words = tweet.split()\n",
    "        uniflag = False\n",
    "        target = ''\n",
    "        targets = []\n",
    "        for word in words:\n",
    "            if word[0] == '@' and len(word) > 1 and not uniflag:\n",
    "                target = word[1:]\n",
    "                uniflag = True\n",
    "            elif word[0] == '@' and len(word) > 1 and uniflag:\n",
    "                targets.append(word[1:])\n",
    "        if len(targets) != 0:\n",
    "            targets.append(target)\n",
    "            multidirected.append([src_list[index], targets, sentiment_list[index]])\n",
    "        elif uniflag:\n",
    "            unidirected.append([src_list[index], target, sentiment_list[index]])\n",
    "    return unidirected, multidirected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After tweet extraction, we have 703311 and 33194 tweets being unidirected and multidirected.\n"
     ]
    }
   ],
   "source": [
    "src_list = biased_dataframe[4].values\n",
    "tweet_list = biased_dataframe[5].values\n",
    "sentiment_list = biased_dataframe[0].values\n",
    "uni_list, multi_list = extract_directed_tweets(src_list, tweet_list, sentiment_list)\n",
    "print(\"After tweet extraction, we have\", len(uni_list), \"and\", len(multi_list), \"tweets being unidirected and multidirected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Validity is unpredictable for multi-directed tweets. First, let us consider only uni-directed tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 293410 distinct source users who tweeted a total of 703311 tweets.\n",
      "These 703311 tweets are directed to among 334272 distinct users.\n",
      "There a total of 96584 common users among these 2 sets.\n"
     ]
    }
   ],
   "source": [
    "src_users = [x[0] for x in uni_list]\n",
    "tgt_users = [x[1] for x in uni_list]\n",
    "src_users_set = set(src_users)\n",
    "tgt_users_set = set(tgt_users)\n",
    "intersection = src_users_set.intersection(tgt_users_set)\n",
    "\n",
    "print(\"There are\", len(src_users_set), \"distinct source users who tweeted a total of\", len(uni_list), \"tweets.\")\n",
    "print(\"These\", len(uni_list), \"tweets are directed to among\", len(tgt_users_set), \"distinct users.\")\n",
    "print(\"There a total of\", len(intersection), \"common users among these 2 sets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a hashmap for a user base of 531098 individuals.\n",
      "Also, 531098 = 703311 + 703311 - 96584\n"
     ]
    }
   ],
   "source": [
    "user_cluster = src_users_set.union(tgt_users_set)\n",
    "user_list = list(user_cluster)\n",
    "hashmap = {user:index for index, user in enumerate(user_list)}\n",
    "print(\"We have a hashmap for a user base of\", len(user_cluster), \"individuals.\")\n",
    "print(\"Also,\", len(user_cluster), \"=\", len(src_users), \"+\", len(tgt_users), \"-\", len(intersection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge count in tuples list is 703311.\n"
     ]
    }
   ],
   "source": [
    "tuples = []\n",
    "for row in uni_list:\n",
    "    if row[2] == 0:\n",
    "        v = -1\n",
    "    elif row[2] == 4:\n",
    "        v = 1\n",
    "    s, t = hashmap[row[0]], hashmap[row[1]]\n",
    "    tuples.append((s, t, v))\n",
    "print(\"Edge count in tuples list is \" + str(len(tuples)) + \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of distinct edge connections is 581815.\n"
     ]
    }
   ],
   "source": [
    "edges = [(x,y) for (x,y,z) in tuples]\n",
    "distinct_edges = set(edges)\n",
    "print(\"The number of distinct edge connections is \" + str(len(distinct_edges)) + \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two funtions execute the following function:\n",
    "- Check if two tuple elements correspond to the same directed edge.\n",
    "- Compute an equivalent for multiple edges between two specific nodes, mentions if resultant edge is rendered neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edges_are_same(a, b):\n",
    "    if a[0:2] == b[0:2]:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def get_equivalent_edge(edges):\n",
    "    mean = 0\n",
    "    for edge in edges:\n",
    "        mean += edge[2]\n",
    "    if mean == 0:\n",
    "        return (edges[0][0], edges[0][1], 0), False\n",
    "    elif mean > 0:\n",
    "        return (edges[0][0], edges[0][1], 1), True\n",
    "    else:\n",
    "        return (edges[0][0], edges[0][1], -1), True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taking majority value of all multiple directed edges between two nodes and ignoring all the edges that are rendered\n",
      "balanced due to positive and negative tweets, we have total of 566173 edges left in the network.\n"
     ]
    }
   ],
   "source": [
    "tuples.sort(key=lambda x: x[0]*1000000 + x[1])\n",
    "final_tuples = []\n",
    "prev_edge = tuples[0]\n",
    "average_flag = False\n",
    "average_set = []\n",
    "for i in range(1, len(tuples)):\n",
    "    curr_edge = tuples[i]\n",
    "    if not edges_are_same(curr_edge, prev_edge) and not average_flag:\n",
    "        final_tuples.append(prev_edge)\n",
    "    elif edges_are_same(curr_edge, prev_edge) and not average_flag:\n",
    "        average_set.append(prev_edge)\n",
    "        average_flag = True\n",
    "    elif edges_are_same(curr_edge, prev_edge) and average_flag:\n",
    "        average_set.append(prev_edge)\n",
    "    elif not edges_are_same(curr_edge, prev_edge) and average_flag:\n",
    "        average_set.append(prev_edge)\n",
    "        equivalent_edge, valid = get_equivalent_edge(average_set)\n",
    "        average_flag = False\n",
    "        average_set = []\n",
    "        if valid:\n",
    "            final_tuples.append(equivalent_edge)\n",
    "    prev_edge = curr_edge\n",
    "if average_flag:\n",
    "    average_set.append(prev_edge)\n",
    "    equivalent_edge, valid = get_equivalent_edge(average_set)\n",
    "    if valid:\n",
    "        final_tuples.append(equivalent_edge)\n",
    "else:\n",
    "    final_tuples.append(prev_edge)\n",
    "    \n",
    "print(\"Taking majority value of all multiple directed edges between two nodes and ignoring all the edges that are rendered\")\n",
    "print(\"balanced due to positive and negative tweets, we have total of\", len(final_tuples), \"edges left in the network.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges in the graph object is 566173.\n",
      "This is less than 581815 because, effectively 'zero' (balanced) edges are ignored.\n",
      "NetworkX graph created, saving...\n",
      "Graph saved.\n"
     ]
    }
   ],
   "source": [
    "G = nx.DiGraph()\n",
    "G.add_weighted_edges_from(final_tuples)\n",
    "\n",
    "print(\"Number of edges in the graph object is \" + str(G.number_of_edges()) + \".\")\n",
    "print(\"This is less than\", len(distinct_edges), \"because, effectively 'zero' (balanced) edges are ignored.\")\n",
    "\n",
    "print(\"NetworkX graph created, saving...\")\n",
    "nx.write_gpickle(G, 'sentiment140.gpickle')\n",
    "print(\"Graph saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
